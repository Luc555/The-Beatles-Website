Aprendizado Não-supervisionado:Ciclo

	●Ambiente: Este é o contexto ou conjunto de dados no qual o sistema de aprendizado está operando. 
	Pode ser qualquer coisa, desde dados do mundo real até informações simuladas.

	●Representação do Ambiente: Antes de começar a aprender, o sistema precisa de uma forma de representar o ambiente. 
	Isso envolve a escolha de características ou variáveis relevantes para a tarefa de aprendizado.

	●Indutor: Este termo geralmente se refere ao algoritmo ou modelo de aprendizado de máquina usado para extrair padrões e informações do ambiente. 
	O indutor é responsável por aprender a partir dos dados fornecidos.

	●Sistema de Aprendizagem: Isso se refere ao sistema global que inclui o indutor e outros componentes necessários para o processo de aprendizado. 
	Pode envolver pré-processamento de dados, gerenciamento de memória, entre outros.

	●Sinal de Ajuste: Este é um feedback fornecido ao sistema para indicar como ele está se saindo. 
	Pode ser usado para ajustar os parâmetros do indutor e melhorar o desempenho do sistema ao longo do tempo.

	●Indutor (novamente): Após receber o sinal de ajuste, o indutor faz os ajustes necessários em seus parâmetros internos. 
	Isso pode envolver a otimização de pesos em uma rede neural, por exemplo.

Aprendizado Não-supervisionado Comparacao entre Supervisionado	
	=================================================================
	|Não Supervisionado						|
	|	|X1|X2|X3|						|
	|	|  |  |  |						|
	|	|  |  |  |						|
	|O objetivo é aprender a diferenciar ou representar os dados	|
	=================================================================
	=================================================================
	|Supervisionado							|
	|	|X1|X2|Xp|						|
	|	|  |  |  |						|
	|	|  |  |  |						|
	|O alvo é aprender a prever Y					|
	=================================================================
	
Aprendizado Não Supervisionado Caracteritica

	● Não exige classificação prévia dos dados em diferentes categorias;
	● Depende apenas dos atributos descritivos (não do alvo);
	● Não há exemplos rotulados da função a ser induzida;
	============================================================
	|Metodologia baseada em Aprendizado de Máquina, na qual o  |
	|algoritmo de aprendizado não recebe feedback do ambiente a|
	|respeito da saída desejada para o atributo-alvo.	   |
	============================================================

Tarefas do Aprendizado Não-supervisionado
	● Associação: avalia o nível de associação entre dados ou conjuntos de dados.
		○ Exemplos de algoritmos incluem o Apriori, FP-Growth, Eclat, entre outros;
	● Exemplos de aplicação: Sistemas de recomendação, e-commerce, plataformas de streaming,etc.;
	● Sumarização: Geração automática de sumários a partir de textos densos:
		○ Medicina: análise de prontuários médicos;
		○ Educação: aprendizado online;
		○ Pesquisa: resumo de artigos científicos;
		○ Mídias audiovisuais, literatura, mercado financeiro, etc;
	● Agrupamento: Particiona o conjunto de dados em grupos de características semelhantes.
		○ Itens em um mesmo grupo devem ser mais semelhantes (em geral) do que itens em
	grupos diferentes.
______________________________________________________________________________________________________________________________________________________________________________________________
Agrupamento (clustering):
	● Agrupamento: um mesmo conjunto de
	  dados pode ser particionado em grupos
	  diferentes.
	●Exemplos: tabela igual pro dois 
	 a) dividido em 3 grupos b) divido em 8 grupos
______________________________________________________________________________________________________________________________________________________________________________________________
O Algoritmo k-Means
	● Algoritmo de agrupamento particional mais simples;
	● Particiona o conjunto de dados em k > 0 grupos, onde k é um parâmetro
	fornecido como entrada;
	● Utiliza um processo iterativo para encontrar uma partição com k grupos
	que minimize um critério de agrupamento;
	● O resultado é uma partição com grupos compactos, ou seja, com
	variância mínima.

O Algoritmo k-Means
	● O critério de agrupamento é dado pelo erro quadrático.

O Algoritmo k-Means:Validação do resultado
	● O que é melhor?
	Não podemos dizer que um agrupamento é melhor

O Algoritmo k-Means:Validação do resultado
	● Validação - Como obter o valor ideal de k?
	● Variância Intracluster:
	● Agrupamentos melhores tem grupos mais compactos;
	● Quanto menor a variância, menor a partição.

O Algoritmo k-Means:Validação do resultado
	● Como obter o valor ideal de k?
		●  Método do cotovelo: Calcula-se a variância
		intracluster para cada valor de k; calcula-se o
		gráfico entre k e o índice. Identifica-se o
		“cotovelo” quando o aumento de k não
		produz efeito significativo no índice; o k
		encontrado tende a ser o melhor;

		● Coeficiente da silhueta: mede o quão similar
		cada ponto em um cluster é dos pontos
		pertencentes aos clusters vizinhos. Varia de
		-1 (pior) a 1 (melhor).

O Algoritmo k-Medoids:Diferenças para o k-Means
	● No K-Means, cada grupo é representado pelo seu centroide,
	que é a média de todos os pontos de dados nesse grupo;
		○ Isso torna o algoritmo suscetível a outliers;

	● No K-Medoids, cada grupo é representado por um dos pontos
	de dados reais dentro desse grupo, chamado de medoide.
		● O medoide é o ponto de dados que minimiza a soma das
		distâncias para todos os outros pontos no mesmo grupo.
		● Isso torna o k-Medoids menos suscetível a outliers que o
		k-Means.
______________________________________________________________________________________________________________________________________________________________________________________________

Clusterização k-Medoids:O algoritmo PAM
	● Inicialização:
		● Selecione aleatoriamente, ou por meio de alguma estratégia de
		inicialização, k medoides iniciais;
	● Atribuição:
		● Atribua cada ponto de dados ao medoide mais próximo com base em uma
		métrica de distância (por exemplo, distância euclidiana, distância de
		Manhattan);
	● Atualização dos medoides:
		● Para cada medoide, calcule a dissimilaridade total (frequentemente
		chamada de custo) somando as distâncias entre o medoide e todos os
		pontos de dados atribuídos a ele;
		● Para cada ponto de dados que não esteja atualmente servindo como
		medoide, troque-o por um dos medoides e calcule a dissimilaridade total
		após a troca;
		● Selecione a troca de medoide que resulta na menor dissimilaridade total
		para cada cluster.
		● Atualize os medoides com os pontos de dados selecionados.
	● Convergência:
		● Repita as etapas de atribuição e atualização até que critérios de
		convergência sejam atendidos. Critérios comuns de convergência incluem
		nenhuma ou mínima mudança nos medoides e mínima mudança na
		dissimilaridade total;
	● Clusterização Final:
		● A clusterização final é determinada pelos medoides selecionados no final
	do processo de otimização;
	● Saída:
		● Retorne os clusters e seus respectivos medoides como resultado da
		clusterização K-Medoids.
______________________________________________________________________________________________________________________________________________________________________________________________

Clusterização k-Medoids:O algoritmo PAM - Exemplo
	Passo 1: 
		Selecione dois medoides = C1 e C2
		[x1 - x2] + [y1 - y2]
	Passo 2:
		Separa o clusters, c1 e do c2, (x,y)
		Calcule o custo total = pege os medoides selecionados (C1,C2) 
		e diminua pelo (x,y)
		Cost(c,x)= E|ci-xi|
		Custo total = resultado do |ci-xi|
	Passo 3:
		Escolhe aleatoriamente um ponto nao-medoide e recalcule o custo
		C1=(xi,yi) e C2=(xi,yi)
		Troca C2 por 0
		Novos medoides
		C1 e 0
		[x1 - x2] + [y1 - y2]
		Separa o clusters, c1 e do 0, (x,y)
		Calcule o custo total = pege os medoides selecionados (C1,0) 
		e diminua pelo (x,y)
		Cost(c,x)= E|ci-xi|
		Custo total Atual = resultado do |ci-xi|
	Passo 4:
		Custo de trocar o medoide C2 por 0
		S = Total do Custo Atual - Total do Custo Anterior 
		Determinar os resultados apos a trocar C2 por 0
______________________________________________________________________________________________________________________________________________________________________________________________

Relembrando:Pontos Negativos do k-Means
	● k-Means é bastante suscetível a problemas quando os clusters são de diferentes
	tamanhos.
	● k-Means é também bastante suscetível a problemas quando os clusters têm formatos
	globulares ou diferentes densidades.
______________________________________________________________________________________________________________________________________________________________________________________________
O algoritmo DBSCAN
	● DBSCAN é um acrônimo para Density-Based Spatial Clustering Applications with Noise;
	● Trabalha com a definição de densidade para realizar os agrupamentos, identificando
	diferentes tamanhos e formatos mesmo com a presença de ruídos e outliers;
	● Não necessita da definição do número k de clusters, como no k-Means;
	● Ao invés, necessita que sejam definidos dois parâmetros:
		● eps: uma medida de distância utilizada para localizar pontos circunvizinhos;
		● minPts: o número mínimo de pontos aglomerados em uma região para
		  considerá-la densa
	● As definições dos parâmetros do DBSCAN podem ser melhor entendidas a partir de
	dois conceitos:
		● Alcançabilidade: um ponto é alcançável por outro ponto se ele está dentro de
		  uma distância ‘eps’ dele;
		● Conectividade: determina que determinados pontos formam um cluster em
		  particular caso haja uma conexão em cadeia. Por exemplo, p e q são pontos que
		  podem ser conectados caso p -> r -> s -> t -> q, onde ‘a -> b’ significa ‘b é vizinho
		  de a’.

O algoritmo DBSCAN - Categorias de pontos após o processode clusterização do DBSCAN:
	● Núcleo (Core): um ponto com pelo
	  menos ‘m’ pontos dentro de uma
	  distância ‘eps’ dele mesmo;
	● Fronteira (Border): um ponto com pelo
	  menos um ponto de núcleo até uma
	  distância eps;
	● Ruído (Noise): um ponto que nem é
	  núcleo nem fronteira.

O algoritmo DBSCAN Etapas
	1. O algoritmo escolhe aleatoriamente um ponto não selecionado no conjunto de dados;
	2. Se houver pelo menos ‘minPts’ dentro de um raio ‘eps’ do ponto escolhido, considere
	   todos os pontos como pertencentes ao mesmo cluster;
	3. Após os clusters são expandidos aplicando recursivamente os cálculos anteriores para
	   cada ponto circunvizinho.

O algoritmo DBSCAN - Pontos Fracos
	● Sensibilidade a Parâmetros: o desempenho do DBSCAN depende da definição correta de
	  parâmetros, como o eps e o número mínimo de pontos minPts. Escolher valores apropriados pode
	  ser desafiador e pode exigir conhecimento do domínio.
		● Heurísticas comuns são minPts ≥ D + 1, onde D representa o número de dimensões. O
		  parâmetro eps pode ser definido a partir da curva do cotovelo de um gráfico de k pontos
		  vizinhos.
	● Escalabilidade: o DBSCAN pode se tornar computacionalmente custoso para conjuntos de dados
	  maiores, pois precisa calcular distâncias entre pares de pontos, resultando em uma complexidade
	  temporal de O(n^2);
	● Ambiguidade de Pontos de Borda: A atribuição de pontos de borda pode ser às vezes ambígua,
	  levando a diferenças nos resultados do clustering dependendo da ordem de processamento. Isso
	  pode tornar o clustering menos determinístico.
______________________________________________________________________________________________________________________________________________________________________________________________
Clusterização Hierárquica
	● Método de aprendizado não supervisionado por agrupamento;
	● Não necessita de uma pré-definição do número de clusters;
	● Fornece uma representação visual dos grupos a partir de um
	  dendograma;
	● O dendograma representa a relação entre os objetos e mostra quais
	  objetos estão agrupados em diferentes níveis.

Clusterização Hierárquica Algoritmo
	1. Considere cada elemento do conjunto de treinamento como um
	   cluster, ou grupo;
	2. Identifique os dois grupos mais próximos entre si;
	3. Una dos grupos criando um novo grupo;
		● Repita os passos 2 e 3 até que exista só um único grupo.

Clusterização Hierárquica	
	● Clusterização Aglomerativa (bottom-up): inicia com n grupos, sendo n
		● número de itens a serem agrupados. O procedimento termina
		  quando apenas um grupo permanecer.
		● Agrupa grupos similares.
	● Clusterização Divisiva (top-down): inicia com um único grupo. O
	  procedimento termina quando são identificados n grupos, sendo n o
	  número total de itens a serem agrupados;
		● Divide grupos heterogêneos.
	● Resultado reproduzível;
	● Alto custo computacional;
	● Escalonamento e tratamento prévio de outliers é recomendado.

Clusterização Hierárquia-Métricas de distância
	● A forma como é medida a dissimilaridade (distância) dos grupos influencia
	  o resultado final do algoritmo.
		● Clusterização por Ligação Mínima: distância mínima entre dois pontos
		  quaisquer dos clusters (single linkage);
		● Clusterização por Ligação Máxima: distância máxima entre dois
		  pontos quaisquer dos clusters (complete linkage);
		● Clusterização por Ligação Média: distância média entre dois pontos
		  quaisquer dos clusters (average linkage).
______________________________________________________________________________________________________________________________________________________________________________________________

Algoritmo Apriori
	● Algoritmo pertencente à classe de algoritmos de associação;
	● Seu objetivo é encontrar elementos que implicam na presença de outros
	  elementos em uma mesma transação. Ou seja, encontrar relacionamentos ou
	  padrões frequentes entre conjuntos de dados;
	● A compra de um produto quando outro é comprado, por exemplo, representa
	  uma regra de associação;
	● Regras de associação são frequentemente utilizadas em áreas como marketing,
	  mas também em outras áreas, como descrever falhas em linhas de
	  comunicação, ações na interface do usuário, entre outros.

Regras de Associação
	● Regras de associação nada mais são do que um método para explorar itens em um
	  conjunto de dados;
	● Define-se os seguintes termos para as regras de associação:
		● I (Itens): conjunto dos seus n atributos {i 1 , i 2 , ..., i n};
		● D (Database): conjunto das m transações {t 1 , t 2 , ..., t m};
		● Toda transação t i é única em D e consiste em um subconjunto dos Itens I;
		● Define-se a regra de associação (X -> Y) , onde X e Y são subconjuntos de I. Eles
		  não podem ter nenhum elemento em comum.
		● X é chamado de antecedente e Y da consequência da Regra.
	● Mesmo um conjunto pequeno pode gerar uma quantidade muito grande de regras.
	● Como identificar as regras mais relevantes?
		● Support (Suporte);
		● Confidence (Confiança);
		● Lift (Alavancagem);
		● Conviction (Convicção).

Métricas
	● Support (Suporte):
		● Este número é a popularidade do item no conjunto de dados estudado. Sendo
		  assim esse número é definido pela quantidade de vezes que o item apareceu
		  em uma transação, dividido pela quantidade de transações existentes no
		  dataset;
		● supp(x) = (número de transações que X aparece) / (número total de
		  transações);
	● Algo importante a ser citado neste momento é o support threshold. Este número
	irá expressar a quantidade mínima que o suporte de um item (ou conjunto de itens)
	deve aparecer para ele se tornar significante.
	● Confidence (Confiança):
		● A confiança é um número que expressa a possibilidade de um item ser
		comprado quando outro item correlato é comprado;
		● Por exemplo, qual a confiança que um cliente irá comprar um hambúrguer
		considerando que ele já comprou cebolas e batatas;
		● A confiança é calculada por meio da seguinte equação:
		=========================================================================
		|conf(X->Y)= supp(X U P)/supp(X)					|
		=========================================================================
	● Conviction (Convicção):
		● Essa medida está interessada em calcular a frequência que X ocorre e Y
		  não ocorre, ou seja, ela está interessada em quando a Regra falha.
		=========================================================================
		|conf(X->Y)= 1- supp(X/P)/1-conf(x->y)					|
		=========================================================================
		● Essa medida varia entre [0, inf). Se Conf(X->Y) for igual a 1, então o
		  denominador da fórmula é zerado e o resultado da Convicção é definido
		  como infinito.
		● Já se o Supp(Y) for igual a 1, ou seja, Y é presente em todas as transações,
		  então Convicção é igual a 0 — não há erro.

Algoritmo Apriori
	● O Apriori trabalha com o conceito de itens frequentes que são os itens do seu conjunto I que
	  têm a pontuação do Support superior a de um limiar (fornecido como hiperparâmetro);
	● É preciso calcular o Support de todas as combinações de itens e extrair um subconjunto de
	  itens frequentes.
	● Os passos do algoritmo Apriori são:
		● Dentro dos Itens I, extraia um subconjunto (I_freq) dos itens que têm o seu Support
		  maior que o limiar;
		● Dentro de I_freq, itere para formar as combinações dos I_freq com I, aplique o limiar e
		  acumule em I_freq;
		● Pare quando não sobrar nenhum item após aplicar o limiar.
______________________________________________________________________________________________________________________________________________________________________________________________
